{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4, Problem 2: Robot localization using particle filtering\n",
    "## Introduction\n",
    "In the lecture we have learned particle filtering, which can be interpreted as a Monte Carlo method for Hidden Markov Models. In this exercise, we learn how to use this method to track a moving robot's position over time. In this setting, we have access to the steering and velocity control inputs. We also have sensors that measure the distances to visible landmarks. The basic principle of particle filtering is then that the population of particles tracks the high-likelihood regions of the robot's position.\n",
    "\n",
    "Your tasks is to complete the missing code. Make sure that all the functions follow the provided interfaces of the functions, i.e. the output of the function exactly matches the description in the docstring.\n",
    "Adding or modifying code outside of the following comment blocks is not required:\n",
    "\n",
    "```\n",
    "##########################################################\n",
    "# YOUR CODE HERE\n",
    ".....\n",
    "##########################################################\n",
    "```\n",
    "\n",
    "After you fill in all the missing code, restart the kernel and re-run all the cells in the notebook. You are **NOT** allowed to using additional 'import'  statements. If you plagiarise even for a single project task, you won't be eligible for the bonus this semester.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# uncomment for multiple output in a cell\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Initialize particles\n",
    "Particles can be initialized by uniformly sampling in the 2D space, or by Gaussian sampling in robot's starting position. However, the latter one is only helpful if you can estimate where the robot's start point is. In this notebook, we have no information about robot's starting position, so we just uniformly sample particles from the 2D space. In this task, you need to sample particles within the bounds of the given 2D space with no landmarks, namely, all randomly sampled particles are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_particles_construction(width, height, N) -> np.ndarray:\n",
    "    \"\"\"Sample particles in 2D space with no landmarks randomly.\n",
    "\n",
    "    Args:\n",
    "        width, height: float, width (x) and height (y) of the area in which the robot moves\n",
    "        N: int, number of particles\n",
    "    \n",
    "    Return:\n",
    "        particles: numpy (np) array with particles centered around the origin with dimension particles.size = [N,2] \n",
    "    \"\"\"\n",
    "    np.random.seed(500) # setting the seed ensures consistent samples in each execution, do not change this\n",
    "    \n",
    "    ###################################################################\n",
    "    # YOUR CODE HERE\n",
    "    particles = np.random.uniform([0,0], [width, height], size=(N, 2))\n",
    "\n",
    "    ###################################################################\n",
    "    \n",
    "    return particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show a example map in which the robot moves:\n",
    "\n",
    "<img src=\"./data/PF_data/img/Map_Rejection_Sampling.png\" alt=\"Encoder\" style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Find valid particles\n",
    "The map above contains many irregulately distributed landmarks shown as blue circles. In this case, not all previous sampled particles are valid since these particles can not located inside the landmarks. In this task you need to find the valid particles, namely, only those particles whose distance to each landmarks' center is larger or equal to the landmarks' corresponding radius is considered as valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_valid_particles(particles, centers, radii):\n",
    "    \"\"\"Given randomly sampled particles, as well as centers and radii of landmarks, decide which particles are valid.\n",
    "    \n",
    "    Args:\n",
    "        particles: the particles we get through random generation in 2D space\n",
    "        centers: coordinate for each centers of landmarks\n",
    "        radii: the radius of cicular landmarks \n",
    "    Return:\n",
    "        valid_particles: the particles that locate outside the circular landmarks\n",
    "    \"\"\"\n",
    "    valid_particles = []\n",
    "\n",
    "    ###################################################################\n",
    "    # YOUR CODE HERE\n",
    "    for count, particle_coord in enumerate(particles):\n",
    "        dis = np.linalg.norm(particle_coord-centers, axis=1, keepdims=True)\n",
    "        if np.all(dis >= radii):\n",
    "            valid_particles.append(particles[count])    \n",
    "    ###################################################################\n",
    "    return np.asarray(valid_particles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion model\n",
    "Now we have got the uniformly sampled particles in the 2D space and also select the valid particles. Since we have access to the velocity and angle of the robot's motion, now we can move the remaining valid particles based on how you predict the real system is behaving with different noise level in the motion model. Then we can have a look at the influence of different noise level. Assuming that our sensors returns the data in a time interval of 0.4s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_particle_pos(particles, v, std):\n",
    "    \"\"\"Predict the motion of next state for each particles given current angles and velocities. Note that the prediction here is noised.\n",
    "    \n",
    "    Args:\n",
    "        particles: the particles we get after rejecting the ones that are not valid\n",
    "        v： 2d array. Each sample with feature [angle, velocity]\n",
    "        std: control the noise of the distance between time steps\n",
    "    \"\"\"\n",
    "    N = len(particles)\n",
    "    dt = 0.4\n",
    "    # add some noise to the distance, the level is controled by std\n",
    "    delta_dist = (v[1] * dt) + (np.random.randn(N) * std) \n",
    "    particles[:, 0] += np.cos(v[0]) * delta_dist\n",
    "    particles[:, 1] += np.sin(v[0]) * delta_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Update the weights of each particle\n",
    "Update the weighting of the particles based on the measurement. Each particle has assigned an estimated position and a weight which denotes how well the estimate matches the measurement. The weights are normalized so they sum to one in order to obtain a probability distribution. Those particles that are closer to the robot will generally have a higher weight than ones far from the robot. In this case, we need to update the weights using the distance to landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_update(particles, weights, dist_r_l, centers, radii, scale_fac, std):\n",
    "    \"\"\"Given the noised distances between robot and the landmarks, update the weights of particles\n",
    "    \n",
    "    Args:\n",
    "        particles: coordinate of particles\n",
    "        weights: weights of particles\n",
    "        dist_r_l: the current distance between robot and landmarks, using L2 norm here\n",
    "        centers: coordinate for each centers of landmarks\n",
    "        radii: the radius of cicular landmarks\n",
    "        \n",
    "        scale_fac: hyperparameters to avoid the underflow of possibilities\n",
    "        std: standard deviation of the normal distribution\n",
    "    \n",
    "    Here we use a normal distribution to generate the possibility of each particle, you may turn to\n",
    "    the stats.norm.pdf function in this task. Remember to divide the distance by scaling factor when\n",
    "    using stats.norm.pdf.\n",
    "    \"\"\"\n",
    "    # in order to avoid round-off to zero, we add eps to each weights\n",
    "    eps = 1.e-100   \n",
    "    \n",
    "    weights.fill(1.)\n",
    "    ###################################################################\n",
    "    # YOUR CODE HERE\n",
    "    for count, center in enumerate(centers):\n",
    "        # distance between the particles and each landmark\n",
    "        dist_p_l = np.linalg.norm(particles-center, axis=1, keepdims=True) - radii[count]\n",
    "        \n",
    "        weights *= stats.norm.pdf(dist_p_l/scale_fac, dist_r_l[count]/scale_fac, std)\n",
    "    ###################################################################\n",
    "\n",
    "    weights += eps\n",
    "    weights /= sum(weights)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Resample Procedures\n",
    "Discard highly impossible particles and replace them with copies of the more possible particles. Accordingly, particles with greater weights survive with higher likelihood than particles with values of small importance. In principle, there are many ways to achieve this, here you can refer to the procedure given in the paper [Resampling in Particle Filtering - Comparison](http://sait.cie.put.poznan.pl/38/SAIT_38_02.pdf) to complete the systematic resampling method, in which the algorithm is described as below:\n",
    "\n",
    "<img src=\"./data/PF_data/img/systematic_resample.png\" alt=\"Encoder\" style=\"width: 400px;\"/>\n",
    "\n",
    "This resampling has a complexity of *O(N)*, and is one of the more readily recommended, because of its simplicity and operation speed. This approach assumes that the range $[0, 1)$ is subdivided in to N equal parts, and the draw occurs in each stratum\n",
    "$u^i \\sim [\\frac{i-1}{N}, \\frac{i}{N})$, particles are selected for replication, such that $u^i \\in [\\sum_p^{j-1}q_p, \\sum_p^{j}q_p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_procedure(weights, u=np.random.uniform()):\n",
    "    \"\"\"Perform the resampling procedure described above\n",
    "    \n",
    "    Args:\n",
    "        weights: the weights to be updated\n",
    "        u: random number which is drawed only once\n",
    "    \n",
    "    Return:\n",
    "        idx: the indices of those remaining particles\n",
    "    \"\"\"\n",
    "    num_weights = len(weights)\n",
    "    \n",
    "    idx = np.zeros(num_weights, 'i')\n",
    "    sum_Q = np.cumsum(weights)\n",
    "\n",
    "    # make N subdivisions, choose positions with a consistent random offset\n",
    "    U = (np.arange(num_weights) + u) / num_weights\n",
    " \n",
    "    ###################################################################\n",
    "    # YOUR CODE HERE\n",
    "    i, j = 0, 0\n",
    "    while i<num_weights and j<num_weights:\n",
    "        if U[i] < sum_Q[j]:\n",
    "            idx[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    ###################################################################\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above takes an array of weights and returns indices of particles that have been chosen. We just need to write a function that performs the resampling from these indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_from_index(particles, weights, idx):\n",
    "    particles[:] = particles[idx]\n",
    "    weights[:] = weights[idx]\n",
    "    weights /= np.sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "Here we are provided with some information about the maps, and also the robot's trajectory as reference. The velocity and distance data should be intepreted as corresponding transition and observability matrix in HMM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_centers = np.load('./data/PF_data/centers/example_centers.npy')\n",
    "landmark_radii = np.load('./data/PF_data/radii/example_radii.npy')\n",
    "\n",
    "angle_velocity = np.load('./data/PF_data/velocity/example_velocity.npy')\n",
    "dist_r_l = np.load('./data/PF_data/distance/example_distance.npy')\n",
    "\n",
    "# You should not touch this data in you implementation\n",
    "input_sequence = np.load('./data/PF_data/trajectory/example_trajectory.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to see how a robot moves on a new map\n",
    "# landmark_centers = np.load('./data/PF_data/centers/centers_0.npy')\n",
    "# landmark_radii = np.load('./data/PF_data/radii/radii_0.npy')\n",
    "\n",
    "# angle_velocity = np.load('./data/PF_data/velocity/velocity_0.npy')\n",
    "# dist_r_l = np.load('./data/PF_data/distance/distance_0.npy')\n",
    "\n",
    "# # You should not touch this data in you implementation\n",
    "# input_sequence = np.load('./data/PF_data/trajectory/trajectory_0.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters\n",
    "In this cell we set some predefined hyperparameters. The `dis_noise_args` contain noise of different level when updating the particles' position in the next time step. The `weight_conv_args` contain the scaling factor and the standard deviation of random distribution when updating the weights of particles, which control the speed of convergency of particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "width_max = 800\n",
    "height_max = 600 \n",
    "particle_num = 500\n",
    "\n",
    "dis_noise_args = {'no noise': 0.0, 'low noise': 1.0, 'high noise': 5.0}\n",
    "weight_conv_args = {'scale': 50, 'std':1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the implemented functions need to be put together in order to perform particle filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_filtering(width, height, N, centers, radii, velocity, distance, noise, weighting):\n",
    "    \"\"\" Main method for particle filtering. Returns the estimated trajectories of N particles.\n",
    "        \n",
    "    Args:\n",
    "        width, height: decide moving regions of the robot\n",
    "        N: number of particles\n",
    "        centers: centers of landmarks\n",
    "        radii: radii of landmarks\n",
    "        distance: measured distances from the robot to the landmarks\n",
    "        noise: noise level used for prediction\n",
    "        weighting: further parameters defined above\n",
    "    Return:\n",
    "        final list of positions of all particles, final particle weights\n",
    "    \n",
    "    \"\"\"\n",
    "    # First we initialize the particles and the particle weights\n",
    "    random_particles = uniform_particles_construction(width, height, N)\n",
    "    valid_particles = find_valid_particles(random_particles, centers, radii)\n",
    "    origin_random_weights = np.ones((len(valid_particles),1))\n",
    "\n",
    "    # Now we need to record the coordinates of moving particles\n",
    "    prediction_particles = [valid_particles]\n",
    "    random_pos = copy.copy(valid_particles)\n",
    "    random_weights = copy.copy(origin_random_weights)\n",
    "\n",
    "    for i in range(len(distance)):\n",
    "        ###################################################################\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        predict_particle_pos(random_pos, velocity[i], noise)\n",
    "        _ = weights_update(random_pos, random_weights, distance[i], centers, radii, weighting['scale'], weighting['std'])\n",
    "        random_index = resample_procedure(random_weights, np.random.uniform())\n",
    "        resample_from_index(random_pos, random_weights, random_index)\n",
    "        prediction_particles.append(copy.copy(random_pos))\n",
    "        ###################################################################\n",
    "    \n",
    "    particle_trajectory = np.asarray(prediction_particles)\n",
    "    return particle_trajectory, random_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_result(width, height, N, centers, radii, v, dist, noise, convergence):\n",
    "    trajectory_all = []\n",
    "    weight_all = []\n",
    "\n",
    "    for noise_i in noise.values():\n",
    "        trajectory, weight = particle_filtering(width, height, N, centers, radii, v, dist, noise_i, convergence)\n",
    "        trajectory_all.append(trajectory)\n",
    "        weight_all.append(weight)\n",
    "\n",
    "    return trajectory_all, weight_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_total, weight_total = concatenate_result(width_max, height_max, particle_num, landmark_centers, landmark_radii, angle_velocity, dist_r_l, dis_noise_args, weight_conv_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Now we visualize the movement of the robot to show how your particle filters works! What we need to achieve here is adding the moving of input sequence as well as particles onto that map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location(step):\n",
    "    img = plt.imread('data/PF_data/img/Canvas.png')\n",
    "    fig,ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax_unpack = ax.ravel()\n",
    "    \n",
    "    for i in range(0, 3):\n",
    "        ax_unpack[i].set_aspect('equal')\n",
    "        ax_unpack[i].scatter(input_sequence[step,0], input_sequence[step,1], s=15, c='r', marker='x')\n",
    "        ax_unpack[i].scatter(trajectory_total[i][step,:,0], trajectory_total[i][step,:,1],s=1, c='g')\n",
    "\n",
    "        for count, value in enumerate(landmark_centers):\n",
    "            circ = mpatches.Circle(value,landmark_radii[count], alpha=0.5)\n",
    "            ax_unpack[i].add_patch(circ)\n",
    "        ax_unpack[i].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4560d123e4eb4a3ca7b317c7959d5424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=25, description='step', max=50), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iplot = ipywidgets.interactive(location, step=(0, len(trajectory_total[1])-1))\n",
    "iplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the final position of different noise level\n",
    "Then let's predict the position of robot's final position and make comparison with the true result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_result(trajectory_all, weight_all, noise, reference):\n",
    "\n",
    "    for count, key in enumerate(noise):\n",
    "        prediction = np.average(trajectory_all[count][-1], weights=weight_all[count].flatten(), axis=0)\n",
    "        print(\"The estimate position with \" + key + \" is {}\".format(prediction))        \n",
    "    print(\"The true final position of robot is {}\".format(reference[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate position with no noise is [618.85110432 480.22242078]\n",
      "The estimate position with low noise is [611.03029821 484.30986893]\n",
      "The estimate position with high noise is [607.02302265 493.04249201]\n",
      "The true final position of robot is [615.12953368 500.55248619]\n"
     ]
    }
   ],
   "source": [
    "compare_result(trajectory_total, weight_total, dis_noise_args, input_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the result\n",
    "We can see that the prediction of final position with no noise is not always the best result. Sometimes the prediction with some noise works better. You can relate it to the the gradient descent method when we want to find the minumum value of a non-convex. For example if we get stuck in some local minimum, performing random walks helps to eacape from this local minima. Same is hold for particle filters with different noise levels. Once the minimum offset for particles with no noise is decided, it will always be there and can never be eliminated. But with different noise level, particles may come to some smaller offset. You can also load another given test data to see the movement on a new map."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
