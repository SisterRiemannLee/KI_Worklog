{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4, Part 2: Robot localization using particle filtering\n",
    "## Introduction\n",
    "In the lecture we have learned particle filtering. which can be interpreted as Monte Carlo method for Hidden Markov Models. In this exercise, we learn how to use this method to track a moving robot's position overtime. Basically, we have access to the steering and velocity control inputs. We also have sensors that measures distance to visible landmarks. Then the basic idea is that the population of particles tracks the high-likelihood regions of the robot's position.\n",
    "\n",
    "Your tasks is to complete the missing code. Make sure that all the functions follow the provided specification, i.e. the output of the function exactly matches the description in the docstring.\n",
    "Do not add or modify any code outside of the following comment blocks:\n",
    "```\n",
    "##########################################################\n",
    "# YOUR CODE HERE\n",
    ".....\n",
    "##########################################################\n",
    "```\n",
    "After you fill in all the missing code, restart the kernel and re-run all the cells in the notebook. You are **NOT** allowed to using additional 'import'  statements. If you plagiarise even for a single project task, you won't be eligible for the bonus this semester.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from scipy import stats\n",
    "from IPython.display import HTML\n",
    "from ipywidgets import interactive\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "# comment these two lines if you don't want multiple output in a cell\n",
    "# just for the convenience of debugging\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Constuct particles\n",
    "Particles can be constructed by uniformly sampling in the 2D space, or by Gaussian sampling in robot's start point. The latter one is more helpful if you know where the robot's start point is. In this notebook, we have no information about robot's start point, so we just randomly sample particles over the 2D space. In this task, you need to sample particles in the given 2D space with no landmarks, namely, all randomly sampled particles is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_particles_construction(width, height, N):\n",
    "    \"\"\"Sample particles in 2D space with no landmarks randomly.\n",
    "\n",
    "    Args:\n",
    "        width, height: int, width and height of the area in which the robot moves\n",
    "        N: int, number of particles\n",
    "    \n",
    "    Return:\n",
    "        particles: np.ndarray, the coordinate of particles in 2D space\n",
    "    \"\"\"\n",
    "    np.random.seed(500)\n",
    "    ###################################################################\n",
    "    # YOUR CODE HERE\n",
    "    particles = np.random.uniform([0,0], [width, height], size=(N, 2))\n",
    "\n",
    "    ###################################################################\n",
    "    \n",
    "    return particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maps with Landmarks\n",
    "Here we show a example map in which the robot moves:\n",
    "\n",
    "<img src=\"./img/Map_Rejection_Sampling.png\" alt=\"Encoder\" style=\"width: 400px;\">\n",
    "\n",
    "which contains many irregulatly distributed landmarks shown as blue circles. In this case, not all previous sampled particles are valid since these particles can not located inside the landmarks.Furthermore, suppose we have a clue which indicates that the robot is more likely to start from the left-half plane of this map. In this case, we should turn to **Rejection Sampling**, which is used to produce samples from a hard-to-sample distribution, given an easy-to-sample distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Rejection Sampling\n",
    "In this task you need to determine the conditinal probability $P(L|V)$, where: \n",
    "- $V$: the sampled particles are valid, which means that they never locate inside the landmarks\n",
    "- $L$: the sampled particles are located in the left-half plane\n",
    "So first, you need to decided which particles are valid among the randomly sampled points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_valid_particles(particles, centers, radius):\n",
    "    \"\"\"Given randomly sampled particles, as well as centers and radii of landmarks, decide which particles are valid.\n",
    "    \n",
    "    Args:\n",
    "        particles: the particles we get through random generation in 2D space\n",
    "        centers: centers of landmarks\n",
    "        radius: the radius of cicular landmarks \n",
    "    Return:\n",
    "        valid_particles: the particles that locate in the valid region\n",
    "    \"\"\"\n",
    "    valid_particles = []\n",
    "\n",
    "    ###################################################################\n",
    "    # YOUR CODE HERE\n",
    "    for count_p, coord_p in enumerate(particles):\n",
    "        dis = np.linalg.norm(coord_p-centers, axis=1, keepdims=True)\n",
    "        if np.all(dis >= radius):\n",
    "            valid_particles.append(particles[count_p])    \n",
    "    ###################################################################\n",
    "    return np.asarray(valid_particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_left_particles(particles, width):\n",
    "    \"\"\"Given valid sampled particles, choose the ones that are on the left-half plane\n",
    "    \n",
    "    Args:\n",
    "        particles: the particles which are valid\n",
    "        width: the width of the map\n",
    "    Return:\n",
    "        left_particles: the particles that locate in left-half plane\n",
    "    \"\"\"\n",
    "    left_particles = []\n",
    "\n",
    "    ###################################################################\n",
    "    # YOUR CODE HERE\n",
    "    for count_p, coord_p in enumerate(particles):\n",
    "        if (coord_p[0] <= width/2):\n",
    "            left_particles.append(particles[count_p])    \n",
    "    ###################################################################\n",
    "    return np.asarray(left_particles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's predefine some parameters and then run some examples to decide the proportion of valid particles that are in the left-half plane. Can you see what's the influence of particle numbers through this example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate proportion of left valid particles in 1000 particles is 0.48063\n",
      "Estimate proportion of left valid particles in 1500 particles is 0.48816\n",
      "Estimate proportion of left valid particles in 2000 particles is 0.49350\n",
      "Estimate proportion of left valid particles in 2500 particles is 0.49043\n",
      "Estimate proportion of left valid particles in 3000 particles is 0.49325\n",
      "Estimate proportion of left valid particles in 3500 particles is 0.49303\n",
      "Estimate proportion of left valid particles in 4000 particles is 0.49586\n",
      "Estimate proportion of left valid particles in 4500 particles is 0.50219\n",
      "Estimate proportion of left valid particles in 5000 particles is 0.50394\n",
      "Estimate proportion of left valid particles in 5500 particles is 0.50160\n",
      "Estimate proportion of left valid particles in 6000 particles is 0.50147\n",
      "Estimate proportion of left valid particles in 6500 particles is 0.50080\n",
      "Estimate proportion of left valid particles in 7000 particles is 0.50266\n",
      "Estimate proportion of left valid particles in 7500 particles is 0.50290\n",
      "Estimate proportion of left valid particles in 8000 particles is 0.50013\n",
      "Estimate proportion of left valid particles in 8500 particles is 0.50195\n",
      "Estimate proportion of left valid particles in 9000 particles is 0.50144\n",
      "Estimate proportion of left valid particles in 9500 particles is 0.50076\n",
      "The true proportion of left valid particles is 0.49884\n"
     ]
    }
   ],
   "source": [
    "[width_max, height_max]= [800, 600]\n",
    "\n",
    "sample_centers = np.array([ [144,73], [510,43], [336,175], [718,159], [178,484], [665,464], [267, 333], [541, 300], [472, 437], [100, 533] ])\n",
    "sample_radii = np.array([ [12], [32], [27], [28], [13], [16], [37], [18], [9], [20] ])\n",
    "\n",
    "if len(sample_centers) != len(sample_radii):\n",
    "    raise ValueError(\"centers and radii must have the same size!\")\n",
    "\n",
    "num_particles = np.arange(1000, 10000, 500)\n",
    "\n",
    "for i in range(len(num_particles)):\n",
    "    origin_particles = uniform_particles_construction(width_max, height_max, num_particles[i])\n",
    "    valid_particles =  find_valid_particles(origin_particles, sample_centers, sample_radii)\n",
    "    remain_particles = find_left_particles(valid_particles, width_max)\n",
    "    print('Estimate proportion of left valid particles in {} particles is {:.5f}'.format(num_particles[i], len(remain_particles[:,0])/len(valid_particles[:,0])))\n",
    "\n",
    "axis_left = np.where(sample_centers[:,0] <= width_max/2)\n",
    "s_left = width_max * height_max / 2 - (np.pi * sample_radii * sample_radii)[axis_left].sum()\n",
    "s_valid = width_max * height_max - (np.pi * sample_radii * sample_radii).sum()\n",
    "print('The true proportion of left valid particles is {:.5f}'.format( s_left / s_valid ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion model\n",
    "Now we have got the uniformly sampled particles in the 2D space. Since we have access to the velocity and angle of the robot's motion, now we can move the remaining valid particles based on how you predict the real system is behaving with different noise level in the motion model. Then we can have a look at the influence of different noise level. Assuming that our sensors returns the data in a time interval of 0.5s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_particle_pos(particles, v, std, dt=0.5):\n",
    "    \"\"\"Predict the motion of next state for each particles given current angles and velocities.\n",
    "    \n",
    "    Args:\n",
    "        particles: the particles we get after rejecting the ones that are not available\n",
    "        v： 2d array. Each sample with feature [angle, velocity]\n",
    "        std: standard deviation of velociy, show the influence of different kind of noise\n",
    "        dt: time interval, assume it to be 0.5 second here\n",
    "    \"\"\"\n",
    "    N = len(particles)\n",
    "    \n",
    "    # std can be set as a hyperparameter to decide how noisy is the data, thus can change difficulty\n",
    "    # add some noise to the distance, the level is control in the parameter dis_noise_args\n",
    "    delta_dist = (v[1] * dt) + (np.random.randn(N) * std) \n",
    "    particles[:, 0] += np.cos(v[0]) * delta_dist\n",
    "    particles[:, 1] += np.sin(v[0]) * delta_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the weights of each particle\n",
    "Update the weighting of the particles based on the measurement. Each particle has a position and a weight which estimates how well it matches the measurement. Normalizing the weights so they sum to one. This normalization step turns them into a probability distribution. Those particles that are closer to the robot will generally have a higher weight than ones far from the robot. Particles that closely match the measurements are weighted higher than particles which don't match the measurements very well. So in this case, we can measure the probability using the distance to landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_update(particles, weights, dist_r_l, centers, radii, scale_fac, std):\n",
    "    \"\"\"Given the noised distances from robot to the landmarks, update the weights of particles\n",
    "    \n",
    "    Args:\n",
    "        particles: coordinate of particles\n",
    "        weights: weight of particles\n",
    "        dist_r_l: the current distance between robot and landmarks\n",
    "        scale_fac, std: hyperparameters to avoid the underflow of possibilities\n",
    "    \"\"\"\n",
    "    \n",
    "    weights.fill(1.)\n",
    "    \n",
    "    for count, center in enumerate(centers):\n",
    "        # distance between the particles and each landmark\n",
    "        dist_p_l = np.linalg.norm(particles-center, axis=1, keepdims=True) - radii[count]\n",
    "        \n",
    "        weights *= stats.norm.pdf(dist_p_l/scale_fac, dist_r_l[count]/scale_fac, std)\n",
    "\n",
    "    weights += 1.e-100   # avoid round-off to zero\n",
    "    weights /= sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample Procedures\n",
    "Discard highly impossible particles and replace them with copies of the more possible particles. Accordingly, particles with greater weights survive with higher likelihood than particles with valus of small importance. In principle, there are many ways to achieve this, here you can refer to the procedure given in the paper [Resampling in Particle Filtering - Comparison](http://sait.cie.put.poznan.pl/38/SAIT_38_02.pdf) to complete the systematic resampling method, in which the algorithm is described as below:\n",
    "\n",
    "<img src=\"img/systematic_resample.png\" alt=\"Encoder\" style=\"width: 400px;\"/>\n",
    "\n",
    "This resampling has a complexity of *O(N)*, and is one of the more readily recommended, because of its simplicity and operation speed. This approach assumes that the range $[0, 1)$ is subdivided in to N equal parts, and the draw occurs in each stratum\n",
    "$u^i \\sim [\\frac{i-1}{N}, \\frac{i}{N})$, particles are selected for replication, such that $u^i \\in [\\sum_p^{j-1}q_p, \\sum_p^{j}q_p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_procedure(weights):\n",
    "    \"\"\"Perform resampling procedure described above\n",
    "    \n",
    "    Args:\n",
    "        weights: the weights to be updated\n",
    "    \n",
    "    ReturnL:\n",
    "        idx: the indices of those remained particled\n",
    "    \"\"\"\n",
    "    \n",
    "    num_weights = len(weights)\n",
    "    \n",
    "    idx = np.zeros(num_weights, 'i') # set the data type as int\n",
    "    sum_Q = np.cumsum(weights)\n",
    "\n",
    "    # make N subdivisions, choose positions with a consistent random offset\n",
    "    U = (np.arange(num_weights) + np.random.uniform()) / num_weights\n",
    " \n",
    "    ###################################################################\n",
    "    # YOUR CODE HERE\n",
    "    i, j = 0, 0\n",
    "    while i<num_weights and j<num_weights:\n",
    "        if U[i] < sum_Q[j]:\n",
    "            idx[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    ###################################################################\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above takes an array of weights and returns indexes of particles that have been chosen. We just need to write a function that performs the resampling from these indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_from_index(particles, weights, idx):\n",
    "    particles[:] = particles[idx]\n",
    "    weights[:] = weights[idx]\n",
    "    weights /= np.sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put the prediction positions of these particles together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the origin data as reference\n",
    "# you should not touch this data in you implementation\n",
    "input_sequence = np.load('./archive/Trajectory_1.npy')\n",
    "# Now let's input the velocity and distance data\n",
    "# So should be intepreted as corresponding transition and observability matrix in HMM type\n",
    "angle_velocity = np.load('./data/velocity_1.npy')\n",
    "dist_r_l = np.load('./data/distance_1.npy')\n",
    "# Now need to figure out what can be done next if you have velocity and diatance data at hand\n",
    "\n",
    "landmark_centers = np.array([ [144,73], [510,43], [336,175], [718,159], [178,484], [665,464], [267, 333], [541, 300], [472, 437], [100, 533] ])\n",
    "landmark_radii = np.array([ [12], [32], [7], [28], [13], [16], [7], [18], [9], [20] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulization\n",
    "Now we visualize the moving of the robot to show how your particle filters works! What we need to achieve here is adding the moving of input sequence as well as particles onto that map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparametes\n",
    "original_particle_number = 500\n",
    "gaussian_particle_std = 100\n",
    "\n",
    "# add noise to the distance prediction\n",
    "dis_noise_args = {'no noise': 0, 'low noise': 1, 'high noise': 5}\n",
    "\n",
    "# decide the convergence of particles\n",
    "weight_conv_args = {'scale': 50, 'std':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def particle_trajectory_record(width, height, N, centers, radii, velocity, distance, noise, weighting):\n",
    "    # First we initialize the particles and the weights\n",
    "    random_particles = uniform_particles_construction(width, height, N)\n",
    "    remaining_particles = find_valid_particles(random_particles, centers, radii)\n",
    "    origin_random_weights = np.ones((len(remaining_particles),1))\n",
    "\n",
    "    # Now we need to record the coordinates of moving particles\n",
    "    prediction_particles = [remaining_particles]\n",
    "\n",
    "    random_pos = copy.copy(remaining_particles)\n",
    "    random_weights = copy.copy(origin_random_weights)\n",
    "\n",
    "    for i in range(len(distance)):\n",
    "\n",
    "        # The predicted position of particles\n",
    "        predict_particle_pos(random_pos, velocity[i], noise, 0.5)\n",
    "        weights_update(random_pos, random_weights, distance[i], centers, radii, weighting['scale'], weighting['std'])\n",
    "        random_index = resample_procedure(random_weights)\n",
    "        resample_from_index(random_pos, random_weights, random_index)\n",
    "        prediction_particles.append(copy.copy(random_pos))\n",
    "    \n",
    "    particle_trajectory = np.asarray(prediction_particles)\n",
    "    return particle_trajectory, random_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_no_noise, weight_no_noise = particle_trajectory_record(width_max, height_max, original_particle_number, landmark_centers, landmark_radii, angle_velocity, dist_r_l, dis_noise_args['no noise'], weight_conv_args)\n",
    "\n",
    "trajectory_low_noise, weight_low_noise = particle_trajectory_record(width_max, height_max, original_particle_number, landmark_centers, landmark_radii, angle_velocity, dist_r_l, dis_noise_args['low noise'], weight_conv_args)\n",
    "\n",
    "trajectory_high_noise, weight_high_noise = particle_trajectory_record(width_max, height_max, original_particle_number, landmark_centers, landmark_radii, angle_velocity, dist_r_l, dis_noise_args['high noise'], weight_conv_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location(step):\n",
    "    img = plt.imread('img/Canvas.png')\n",
    "    fig,ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax_unpack = ax.ravel()\n",
    "\n",
    "    # Now, loop through coord arrays, and create a circle at center\n",
    "    for i in range(0, 3):\n",
    "        for count, value in enumerate(landmark_centers):\n",
    "            circ = Circle(value,landmark_radii[count])\n",
    "            ax_unpack[i].add_patch(circ)\n",
    "    \n",
    "    ax_unpack[0].scatter(input_sequence[step,0], input_sequence[step,1], s=15, c='r', marker='x')\n",
    "    ax_unpack[0].scatter(trajectory_no_noise[step,:,0], trajectory_no_noise[step,:,1],s=1, c='g')\n",
    "\n",
    "    ax_unpack[1].scatter(input_sequence[step,0], input_sequence[step,1], s=15, c='r', marker='x')\n",
    "    ax_unpack[1].scatter(trajectory_low_noise[step,:,0], trajectory_low_noise[step,:,1],s=1, c='g')\n",
    "\n",
    "    ax_unpack[2].scatter(input_sequence[step,0], input_sequence[step,1], s=15, c='r', marker='x')\n",
    "    ax_unpack[2].scatter(trajectory_high_noise[step,:,0], trajectory_high_noise[step,:,1],s=1, c='g')\n",
    "    \n",
    "    plt.xlim(0, width_max) \n",
    "    plt.ylim(0, height_max)\n",
    "\n",
    "    # Create a figure. Equal aspect so circles look circular\n",
    "    ax_unpack[0].set_aspect('equal')\n",
    "    ax_unpack[1].set_aspect('equal')\n",
    "    ax_unpack[2].set_aspect('equal')\n",
    "\n",
    "    ax_unpack[0].imshow(img)\n",
    "    ax_unpack[1].imshow(img)\n",
    "    ax_unpack[2].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6f039a42ae4f018fc7dc133e5069af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=17, description='step', max=35), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iplot = interactive(location, step=(0, len(trajectory_no_noise)-1))\n",
    "iplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's predict the position of robot's final position and make comparison with the true result. The final position should be somewhere around ...(tbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate position with no noise is [169.09956321 372.01721179]\n",
      "The estimate position with low lowise is [170.99563372 369.08019587]\n",
      "The estimate position with high noise is [197.35441023 380.90671861]\n",
      "The true final position of robot is [171.60621762 373.75690608]\n"
     ]
    }
   ],
   "source": [
    "position_no_noise = np.average(trajectory_no_noise[-1], weights=weight_no_noise.flatten(), axis=0)\n",
    "position_low_noise = np.average(trajectory_low_noise[-1], weights=weight_low_noise.flatten(), axis=0)\n",
    "position_high_noise = np.average(trajectory_high_noise[-1], weights=weight_high_noise.flatten(), axis=0)\n",
    "print(\"The estimate position with no noise is {}\".format(position_no_noise))\n",
    "print(\"The estimate position with low lowise is {}\".format(position_low_noise))\n",
    "print(\"The estimate position with high noise is {}\".format(position_high_noise))\n",
    "print(\"The true final position of robot is {}\".format(input_sequence[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding\n",
    "We can see that the prediction of x-axis in low noise is somewhat better than the prediction with no noise. You can relate it to the the gradient descent method when we want to find the minumum value. Sometimes if we get stuck in some local minimum, performing random walks helps to eacape from this local minima. Same is hold for particle filters with some noise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
