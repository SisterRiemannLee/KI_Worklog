{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robot localization using particle filter\n",
    "The robot has steering and velocity control inputs. It has sensors that measures distance to visible landmarks. Both the sensors and control mechanism have noise in them, and we need to track the robot's position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from scipy import stats\n",
    "from IPython.display import HTML\n",
    "from ipywidgets import interactive\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "# use this command for the visulization of animation, uncomment otherwise\n",
    "# %matplotlib notebook\n",
    "\n",
    "# uncomment these two lines if you don't want multiple output in a cell\n",
    "# just for the convenience of debugging\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constuct particles randomly\n",
    "Particles can be constructed by randomly sampling in the 2D space, or by Gaussian sampling in place nearby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Uniform_Particles_Construction(width, height, N):\n",
    "\n",
    "    # set the random seed so that we have reproducible experiments\n",
    "    np.random.seed(500)\n",
    "\n",
    "    particles = np.random.uniform([0,0], [width, height], size=(N, 2))\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reject sampled particles\n",
    "Since previous we get particles randomly among the total 2D spaces, it is possible that there are some particles located inside\n",
    "landmarks, thus we neet to delete these invalid particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rejection_Particles(particles, centers, radius):\n",
    "    \"\"\"Given randomly sampled particles and centers of landmarks, perform rejection here\n",
    "    \n",
    "    Args:\n",
    "        particles: the particles we get through random generation in 2D space\n",
    "        centers: centers of landmarks\n",
    "        radius: the radius of cicular landmarks \n",
    "    \"\"\"\n",
    "    particles_after_rejection = []\n",
    "    for count_p, coord_p in enumerate(particles):\n",
    "        dis = np.linalg.norm(coord_p-centers, axis=1, keepdims=True)\n",
    "        if np.all(dis >= radius):\n",
    "            particles_after_rejection.append(particles[count_p])\n",
    "    return np.asarray(particles_after_rejection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's predefine some parameters and then run two examples to see the influence of number of evidence. What do you see in these two examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated acceptability for 1000 particles is 0.99700\n",
      "Estimated acceptability for 2000 particles is 0.99750\n",
      "Estimated acceptability for 3000 particles is 0.99767\n",
      "Estimated acceptability for 4000 particles is 0.99750\n",
      "Estimated acceptability for 5000 particles is 0.99700\n",
      "Estimated acceptability for 6000 particles is 0.99717\n",
      "Estimated acceptability for 7000 particles is 0.99657\n",
      "Estimated acceptability for 8000 particles is 0.99625\n",
      "Estimated acceptability for 9000 particles is 0.99644\n",
      "The true acceptability is 0.99585\n"
     ]
    }
   ],
   "source": [
    "[Width_Max, Height_Max]= [800, 600]\n",
    "\n",
    "# Landmark center coordinates\n",
    "\n",
    "# Version 1: 5 landmarks\n",
    "Centers = np.array([ [336,175], [718,159], [510,43], [167, 333], [472, 437] ])\n",
    "Radius=np.array([[12],[6],[7],[18],[9]])\n",
    "\n",
    "# Version 2: ten landmarks\n",
    "# Centers = np.array([ [144,73], [510,43], [336,175], [718,159], [178,484], [665,464], [267, 333], [541, 300], [472, 437], [100, 533] ])\n",
    "# Radius=np.array([[12],[32],[7],[8],[13],[6],[7],[8],[9],[10]])\n",
    "\n",
    "if len(Centers) != len(Radius):\n",
    "    raise ValueError(\"Centers and Radius must have the same size!\")\n",
    "\n",
    "Num_Particles = np.arange(1000, 10000, 1000)\n",
    "\n",
    "for i in range(len(Num_Particles)):\n",
    "    particles = Uniform_Particles_Construction(Width_Max, Height_Max, Num_Particles[i])\n",
    "    rejection =  Rejection_Particles(particles, Centers, Radius)\n",
    "    print('Estimated acceptability for {} particles is {:.5f}'.format(Num_Particles[i], len(rejection[:,0])/len(particles[:,0])))\n",
    "\n",
    "print('The true acceptability is {:.5f}'.format(1 - (np.pi * Radius * Radius).sum() / (Width_Max * Height_Max) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion model\n",
    "Now we can move the remaining particles based on how you predict the real system is behaving with some noise in the motion model. Set the time interval to 0.5s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(particles, v, std=1, dt=0.5):\n",
    "    \"\"\"Predict the motion of next state for each particles given current angles and velocities.\n",
    "    \n",
    "    Args:\n",
    "        particles: the particles we get after rejecting the ones that are not available\n",
    "        v： 2d array. Each sample with feature [angle, velocity]\n",
    "        std: standard deviation of velociy, defaut 1\n",
    "        dt: time interval, assume it to be 1 second here\n",
    "    \"\"\"\n",
    "    N = len(particles)\n",
    "    \n",
    "    # add some noise to the distance\n",
    "    # std can be set as a hyperparameter to decide how noisy is the data\n",
    "    # thus we can change the difficulty and different version of the notebook\n",
    "    delta_dist = (v[1] * dt) + (np.random.randn(N) * std)\n",
    "    particles[:, 0] += np.cos(v[0]) * delta_dist\n",
    "    particles[:, 1] += np.sin(v[0]) * delta_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the weights of each particle\n",
    "Update the weighting of the particles based on the measurement. Each particle has a position and a weight which estimates how well it matches the measurement. Normalizing the weights so they sum to one. This normalization step turns them into a probability distribution. Those particles that are closer to the robot will generally have a higher weight than ones far from the robot. Particles that closely match the measurements are weighted higher than particles which don't match the measurements very well. So in this case, we can measure the probability using the distance to landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Weights_Update(particles, weights, coord_rob, centers, radius, scale_fac, std):\n",
    "def Weights_Update(particles, weights, dist_r_l, centers, radius, scale_fac, std):\n",
    "    \"\"\"Given the noised distances from robot to the landmarks, update the weights of particles\n",
    "    \n",
    "    Args:\n",
    "        particles: coordinate of particles\n",
    "        weights: weight of particles\n",
    "        dist_r_l: the current distance between robot and landmarks\n",
    "        scale_fac, std: hyperparameters to avoid the underflow of possibilities\n",
    "    \"\"\"\n",
    "    \n",
    "    weights.fill(1.)\n",
    "    \n",
    "    for count, center in enumerate(centers):\n",
    "        # distance between the particles and each landmark\n",
    "        dist_p_l = np.linalg.norm(particles-center, axis=1, keepdims=True) - radius[count]\n",
    "        \n",
    "        # have tried use exponential function to avoid underflow, but still of no use\n",
    "        # so here use scale_fac and std to avoid the underflow of possibilities\n",
    "        # set the distance as mean and std as standard deviation of norm distribution, then get the pdf as our new weights\n",
    "        weights *= stats.norm.pdf(dist_p_l/scale_fac, dist_r_l[count]/scale_fac, std)\n",
    "\n",
    "    weights += 1.e-300   # avoid round-off to zero\n",
    "    weights /= sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample Procedures\n",
    "Discard highly impossible particles and replace them with copies of the more possible particles. Here you can refer to the procedure given as below:\n",
    "<img src=\"img/Resample_Proedure.png\" alt=\"Encoder\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def systematic_resample(weights):\n",
    "    \n",
    "    Num_Weights = len(weights)\n",
    "    \n",
    "    # make N subdivisions, choose positions with a consistent random offset\n",
    "    delta_plus = (np.arange(Num_Weights) + np.random.random()) / Num_Weights\n",
    " \n",
    "    idx = np.zeros(Num_Weights, 'i') # set the data type as int\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    \n",
    "    i, j = 0, 0\n",
    "    while i<Num_Weights and j<Num_Weights:\n",
    "        if delta_plus[i] < cumulative_sum[j]:\n",
    "            idx[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above takes an array of weights and returns indexes of particles that have been chosen. We just need to write a function that performs the resampling from these indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_from_index(particles, weights, idx):\n",
    "    particles[:] = particles[idx]\n",
    "    weights[:] = weights[idx]\n",
    "    weights /= np.sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put the prediction positions of these particles together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Sequence = np.load('./archive/Trajectory_1.npy')\n",
    "# Now let's input the velocity and distance data\n",
    "# So should be intepreted as corresponding transition and observability matrix in HMM type\n",
    "Angle_Velocity = np.load('./data/velocity_1.npy')\n",
    "Dist_r_l = np.load('./data/distance_1.npy')\n",
    "# Now need to figure out what can be done next if you have velocity and diatance data at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(49, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(49, 5, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Input_Sequence)\n",
    "np.shape(Angle_Velocity)\n",
    "np.shape(Dist_r_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we get the particles\n",
    "random_particles = Uniform_Particles_Construction(Width_Max, Height_Max, 50)\n",
    "reject_particles = Rejection_Particles(random_particles, Centers, Radius)\n",
    "Origin_Weights = np.ones((len(reject_particles),1))\n",
    "\n",
    "# Now we need to record the coordinates of moving particles\n",
    "Prediction_Paticles = [reject_particles]\n",
    "\n",
    "Pos = copy.copy(reject_particles)\n",
    "Weights = copy.copy(Origin_Weights)\n",
    "\n",
    "for i in range(len(Input_Sequence)-1):\n",
    "\n",
    "    # The predicted position of particles\n",
    "    Predict(Pos, Angle_Velocity[i], 1, 0.5)\n",
    "    Weights_Update(Pos, Weights, Dist_r_l[i], Centers, Radius, 50, 5)\n",
    "    Index = systematic_resample(Weights)\n",
    "    resample_from_index(Pos, Weights, Index)\n",
    "    Prediction_Paticles.append(copy.copy(Pos))\n",
    "    \n",
    "Particle_Trajectory = np.asarray(Prediction_Paticles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulization\n",
    "Now we visualize the moving of the robot to show how your particle filters works! What we need to achieve here is adding the moving of input sequence as well as particles onto that map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Location(step):\n",
    "    img = plt.imread('img/Canvas.png')\n",
    "    fig,ax = plt.subplots(1)\n",
    "\n",
    "    # Now, loop through coord arrays, and create a circle at center\n",
    "    for count, value in enumerate(Centers):\n",
    "        circ = Circle(value,Radius[count])\n",
    "        ax.add_patch(circ)\n",
    "    \n",
    "    ax.scatter(Input_Sequence[step,0], Input_Sequence[step,1], s=6, c='r')\n",
    "    ax.scatter(Particle_Trajectory[step,:,0], Particle_Trajectory[step,:,1],s=3, c='g')\n",
    "    \n",
    "    plt.xlim(0, Width_Max) \n",
    "    plt.ylim(0, Height_Max)\n",
    "\n",
    "    # Create a figure. Equal aspect so circles look circular\n",
    "    ax.set_aspect('equal')\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6670937fe770439aafbab60775754c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=24, description='step', max=49), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iplot = interactive(Location, step=(0, len(Particle_Trajectory)-1))\n",
    "iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Writer = animation.PillowWriter(fps=10)\n",
    "ani.save(\"gif/Random_Particle_Filter_1.gif\", writer=Writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the robot starts at zero point, but the particles sampled randomly in the 2D spaces rarely go close to the zero, hence cause the bias which exists all over the trajectory. So instead, let's try with the Gaussian sample menthod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_Particles_Construction(N, sigma):\n",
    "    particles = sigma * np.random.randn(N, 2)\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_particles = Gaussian_Particles_Construction(50, 5)\n",
    "reject_particles = Rejection_Particles(gaussian_particles, Centers, Radius)\n",
    "Origin_Weights = np.ones((len(reject_particles),1))\n",
    "\n",
    "# Now we need to record the coordinates of moving particles\n",
    "Prediction_Paticles = [reject_particles]\n",
    "\n",
    "Pos = copy.copy(reject_particles)\n",
    "Weights = copy.copy(Origin_Weights)\n",
    "\n",
    "for i in range(len(Input_Sequence)-1):\n",
    "\n",
    "    Predict(Pos, v[i], 1, 0.5)\n",
    "    Weights_Update(Pos, Weights, Input_Sequence[i], Centers, Radius, 50, 5)\n",
    "    Index = systematic_resample(Weights)\n",
    "    resample_from_index(Pos, Weights, Index)\n",
    "    Prediction_Paticles.append(copy.copy(Pos))\n",
    "    \n",
    "Particle_Trajectory = np.asarray(Prediction_Paticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('img/Canvas.png')\n",
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "# Now, loop through coord arrays, and create a circle at center\n",
    "for count, value in enumerate(Centers):\n",
    "    circ = Circle(value,Radius[count])\n",
    "    ax.add_patch(circ)\n",
    "\n",
    "# Create a figure. Equal aspect so circles look circular\n",
    "ax.set_aspect('equal')\n",
    "ax.imshow(img)\n",
    "\n",
    "# Initialization\n",
    "line, = ax.plot(Input_Sequence[1,0], Input_Sequence[1,1], 'r')\n",
    "scat = ax.scatter(Particle_Trajectory[0,:,0], Particle_Trajectory[0,:,1],s=3, c='g')\n",
    "\n",
    "plt.xlim(0, Width_Max) \n",
    "plt.ylim(0, Height_Max) \n",
    "\n",
    "# ani = animation.FuncAnimation(fig, animate, np.arange(1, len(Input_Sequence)-1), init_func=init, interval=500, blit=True) \n",
    "ani = animation.FuncAnimation(fig, animate, np.arange(1,len(Input_Sequence)-1), init_func=init, interval=150, blit=True) \n",
    "_ = HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save(\"gif/Gaussian_Particle_Filter_1.gif\", writer=Writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
