{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robot localization using particle filter\n",
    "The robot has steering and velocity control inputs. It has sensors that measures distance to visible landmarks. Both the sensors and control mechanism have noise in them, and we need to track the robot's position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation \n",
    "\n",
    "from IPython.display import HTML\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "# set print options\n",
    "# np.set_printoptions(threshold=3)\n",
    "# np.set_printoptions(suppress=True)\n",
    "# np.set_printoptions(precision=2)\n",
    "%matplotlib notebook\n",
    "\n",
    "# Use this for debugging, show multiply outputs in one cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constuct particles randomly\n",
    "Particles can be constructed by randomly sampling in the 2D space. Each particle has a weight (probability) indicating how likely it matches the actual state of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Uniform_Particles_Construction(width, height, N):\n",
    "    particles = np.zeros((N, 2))\n",
    "    # set the random seed so that we have reproducible experiments\n",
    "    np.random.seed(500)\n",
    "    # randomly draw particles from the 2D space, type ndarray\n",
    "    particles = np.random.uniform([0,0], [width, height], size=(N, 2))\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejection sampling\n",
    "Since previous we get particles randomly among the total 2D spaces, it is possible that there are some particles located in the landmarks, thus we neet to delete these invalid particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rejection_Sampling(particles, centers, radius):\n",
    "    \"\"\"Given randomly sampled particles and centers of landmarks, perform rejection here\n",
    "    \n",
    "    Args:\n",
    "        particles: the particles we get through random generation in 2D space\n",
    "        centers: centers of landmarks\n",
    "        radius: the radius of cicular landmarks \n",
    "    \"\"\"\n",
    "    particles_after_rejection = []\n",
    "    for count_p, coord_p in enumerate(particles):\n",
    "        dis = np.linalg.norm(coord_p-centers, axis=1, keepdims=True)\n",
    "        if np.all(dis >= radius):\n",
    "            particles_after_rejection.append(particles[count_p])\n",
    "    return np.asarray(particles_after_rejection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's predefine some parameters and then run two examples to see the influence of number of evidence. What do you see in these two examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated acceptability for 1000 particles is 0.98600\n",
      "Estimated acceptability for 1500 particles is 0.98667\n",
      "Estimated acceptability for 2000 particles is 0.98700\n",
      "Estimated acceptability for 2500 particles is 0.98720\n",
      "Estimated acceptability for 3000 particles is 0.98700\n",
      "Estimated acceptability for 3500 particles is 0.98743\n",
      "Estimated acceptability for 4000 particles is 0.98825\n",
      "Estimated acceptability for 4500 particles is 0.98844\n",
      "Estimated acceptability for 5000 particles is 0.98840\n"
     ]
    }
   ],
   "source": [
    "[Width_Max, Height_Max]= [800, 600]\n",
    "\n",
    "# Landmark center coordinates\n",
    "Centers = np.array([ [336,175], [718,159], [510,43], [167, 333], [472, 437] ])\n",
    "Radius=[12,6,7,18,9]\n",
    "if len(Centers) != len(Radius):\n",
    "    raise ValueError(\"Centers and Radius must have the same size!\")\n",
    "\n",
    "Num_Particles = np.arange(1000, 5500, 500)\n",
    "\n",
    "for i in range(len(Num_Particles)):\n",
    "    particles = Uniform_Particles_Construction(Width_Max, Height_Max, Num_Particles[i])\n",
    "    rejection =  Rejection_Sampling(particles, Centers, Radius)\n",
    "    print('Estimated acceptability for {} particles is {:.5f}'.format(Num_Particles[i], len(rejection[:,0])/len(particles[:,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion model\n",
    "Now we can move the remaining particles based on how you predict the real system is behaving with some noise in the motion model. Assume the time interval here is 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(particles, v, std=1, dt=0.5):\n",
    "    \"\"\"Predict the motion of next state for each particles given current angles and velocities.\n",
    "    \n",
    "    Args:\n",
    "        particles: the particles we get after rejection\n",
    "        vï¼š 2d array. Each sample with feature [angle, velocity]\n",
    "        std: standard deviation of velociy, defaut 1\n",
    "        dt: time interval, assume it to be 1 second here\n",
    "    \"\"\"\n",
    "    N = len(particles)\n",
    "    \n",
    "    delta_dist = (v[1] * dt) + (np.random.randn(N) * std)\n",
    "    particles[:, 0] += np.cos(v[0]) * delta_dist\n",
    "    particles[:, 1] += np.sin(v[0]) * delta_dist\n",
    "    \n",
    "#     return particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how should we get the velocity of the robot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compute_Velocity(Trajectory, delta_t=0.5):\n",
    "    \"\"\"Given the trajectory of robots, computer corresponding direction and velocity\n",
    "    \n",
    "    Args:\n",
    "        Trajectory: np.ndarray, the coordinate of robot\n",
    "        delta_t: int, default 0.5, depend on screen shot interval\n",
    "    \n",
    "    Returns:\n",
    "        angle_and_velocity: np.ndarray in the form like [angel, velocity]\n",
    "    \n",
    "    \"\"\"\n",
    "    angle_and_velocity = []\n",
    "\n",
    "    for i in range(len(Trajectory)-1):\n",
    "        angle = np.arctan2(Trajectory[i+1, 1] - Trajectory[i, 1], Trajectory[i+1, 0] - Trajectory[i, 0])\n",
    "        velocity = np.linalg.norm(Trajectory[i+1] - Trajectory[i]) / delta_t\n",
    "        angle_and_velocity.append([angle, velocity])\n",
    "        \n",
    "    return angle_and_velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the weights of each particle\n",
    "Update the weighting of the particles based on the measurement. Each particle has a position and a weight which estimates how well it matches the measurement. Normalizing the weights so they sum to one turns them into a probability distribution. The particles those that are closest to the robot will generally have a higher weight than ones far from the robot. Particles that closely match the measurements are weighted higher than particles which don't match the measurements very well. So in this case, we can measure the probability using the distance to landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weights_Update(particles, weights, coord_rob, std, centers, radius):\n",
    "    # seems also like some initialization here?\n",
    "    weights.fill(1.)\n",
    "    for count, coord_cen in enumerate(centers):\n",
    "        # Note that the method enumerate is typically used in list type\n",
    "        # distance between the robot and each landmark\n",
    "        distance = np.linalg.norm(coord_rob-coord_cen, axis=1) - radius[count]\n",
    "        # set the distance as mean and R as standard deviation of \n",
    "        # norm distribution, then get the pdf as our new weights\n",
    "        \n",
    "        weights *= sp.stats.norm(distance, R).pdf(trajectory[count])\n",
    "\n",
    "    weights += 1.e-300   # avoid round-off to zero\n",
    "    weights /= sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't resample at every epoch. For example, if you received no new measurements you have not received any information from which the resample can benefit. We can determine when to resample by using something called the effective N, which approximately measures the number of particles which meaningfully contribute to the probability distribution. The equation for this is\n",
    "$$\\hat{N}_{eff} = \\frac{1}{\\Sigma w^2}$$\n",
    "thus, we complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neff(weights):\n",
    "    return 1. / np.sum(np.square(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample Procedures\n",
    "Discard highly improbable particle and replace them with copies of the more probable particles. Here you can refer to the procedure given as below:\n",
    "<img src=\"img/Resample_Proedure.png\" alt=\"Encoder\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def systematic_resample(weights):\n",
    "    N = len(weights)\n",
    "    delta_plus = (np.arange(N) + np.random.random()) / N\n",
    " \n",
    "    idx = np.zeros(N, 'i')\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    i, j = 0, 0\n",
    "    while i<N and j<N:\n",
    "        if positions[i] < cumulative_sum[j]:\n",
    "            idx[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, compute weighted mean and covariance of the set of particles to get a state estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate(particles, weights):\n",
    "    pos = particles[:, 0:1]\n",
    "    mean = np.average(pos, weights=weights, axis=0)\n",
    "    var = np.average((pos - mean)**2, weights=weights, axis=0)\n",
    "    return mean, var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They take an array of weights and returns indexes to the particles that have been chosen for the resampling. We just need to write a function that performs the resampling from these indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_from_index(particles, weights, indexes):\n",
    "    particles[:] = particles[indexes]\n",
    "    weights[:] = weights[indexes]\n",
    "    weights /= np.sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulization\n",
    "Now we visualize the moving of the robot to show how your particle filters works! What we need to achieve here is adding the moving of input sequence as well as particles onto that map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's put the prediction positions of these particles together\n",
    "# First get the input and compute the velocity\n",
    "\n",
    "Input_Sequence = np.load('Trajectory.npy')\n",
    "v = Compute_Velocity(Input_Sequence, 0.5)\n",
    "\n",
    "# then we get the particles\n",
    "random_particles = Uniform_Particles_Construction(Width_Max, Height_Max, 50)\n",
    "reject_particles = Rejection_Sampling(particle_orig, Centers, Radius)\n",
    "Origin_Weights = np.ones(len(reject_particles))\n",
    "\n",
    "# Now we need to record the coordinates of moving particles\n",
    "Prediction_Paticles = [reject_particles]\n",
    "\n",
    "Pos = copy.copy(reject_particles)\n",
    "for i in range(len(Input_Sequence)-1):\n",
    "    Predict(Pos, v[i], 1, 0.5)\n",
    "    Prediction_Paticles.append(copy.copy(Pos))\n",
    "Particle_Trajectory = np.asarray(Prediction_Paticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(i): \n",
    "    line.set_data(Input_Sequence[:i+1,:].T) # update the data \n",
    "    scat.set_offsets(Particle_Trajectory[i])\n",
    "    return line, scat,\n",
    "\n",
    "# Init only required for blitting to give a clean slate. \n",
    "def init(): \n",
    "    line.set_data(Input_Sequence[1,0], Input_Sequence[1,1])\n",
    "    scat.set_offsets(Particle_Trajectory[0,:,:])\n",
    "    return line, scat, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('img/Canvas.png')\n",
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "# Now, loop through coord arrays, and create a circle at center\n",
    "for count, value in enumerate(Centers):\n",
    "    circ = Circle(value,Radius[count])\n",
    "    ax.add_patch(circ)\n",
    "\n",
    "# Create a figure. Equal aspect so circles look circular\n",
    "ax.set_aspect('equal')\n",
    "ax.imshow(img)\n",
    "\n",
    "# Initialization\n",
    "line, = ax.plot(Input_Sequence[1,0], Input_Sequence[1,1], 'r')\n",
    "scat = ax.scatter(Particle_Trajectory[0,:,0], Particle_Trajectory[0,:,1],s=3, c='g')\n",
    "\n",
    "plt.xlim(0, Width_Max) \n",
    "plt.ylim(0, Height_Max) \n",
    "\n",
    "# ani = animation.FuncAnimation(fig, animate, np.arange(1, len(Input_Sequence)-1), init_func=init, interval=500, blit=True) \n",
    "ani = animation.FuncAnimation(fig, animate, np.arange(1,len(Input_Sequence)-1), init_func=init, interval=500, blit=True) \n",
    "_ = HTML(ani.to_html5_video())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
